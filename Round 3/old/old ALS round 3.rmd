---
output:
  html_document:
        theme: readable
editor_options: 
  chunk_output_type: console
---

```{r}

library(glmnet)
library(gbm)
library(tidyverse)
library(cowplot)
library(caret)
library(mlbench)

df <- read_rds(file = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")  %>% tbl_df()

df <- df %>% rename(response = dFRS)

head(df)

```

```{r, message=F, warning=F}

text_base_size   <- 10

ggplot_text_size <- text_base_size / ggplot2::.pt # Trick to make geom_text etc. same size and theme

theme_set(theme_cowplot(font_size = text_base_size, rel_small = 1, rel_tiny = 1, rel_large = 1))

knitr::opts_chunk$set(fig.width = 120/25.4)
knitr::opts_chunk$set(fig.height = 80/25.4)
knitr::opts_chunk$set(dpi = 200)

```

# Split into training and prediction set

```{r}

set.seed(0)

data_train    <- df %>% filter(!is.na(response))
data_train    <- na.omit(data_train)
data_predict  <- df %>% filter(is.na(response))                              

dim(data_predict)
dim(data_train)

```

## Split in train/test

```{r}

set.seed(1)

inTraining <- createDataPartition(data_train$response, p = .90, list = FALSE) # We use 90% of data for training

training    <- data_train[ inTraining,]
validation  <- data_train[-inTraining,]

x <- getModelInfo(model = "glmnet")[[1]]
x$label
x$type
x$parameters

training_x <- training %>% select(-response) %>% as.data.frame()
training_y <- training$response

```

```{r}

trControl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, verboseIter = F)
  
fit <- train(x = training_x, 
             y = training_y, 
        method = "glmnet",
     trControl = trControl
    )
  
print(fit)
  
ggplot(fit)

# alpha = 0.55   lambda = 0.0400407611  RMSE = 0.5054235

```


```{r}

trControl <- trainControl(method = "repeatedcv", number = 5, repeats = 5, verboseIter = F)
  
#install.packages("dismo")
library(dismo)


# TODO: Find a way to test multiple values for n.trees (118), or just try a few and choose 1
f <- gbm.step(data = training, gbm.x = 3:370, gbm.y = 2, step.size = 5, max.trees = 130, n.trees = 10, family = "gaussian")


fit <- train(x = training_x, 
             y = training_y, 
        method = "gbm",
     trControl = trControl,
     n.trees = c(90,91)
    )
  
print(fit)
  
ggplot(fit)


# We predict on the test fold
predicted <- predict(fit, newdata = validation)

# We compare with the observed values and calculate RMSE
observed  <- validation$response
#mse       <- mean((observed-predicted)^2)
#rmse     <- sqrt(mse)

test_rmse = 

#test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)

test_rmse


```

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode' was
 held constant at a value of 10
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 100, interaction.depth = 1, shrinkage = 0.1 and
n.minobsinnode = 10.
interaction.depth  n.trees  RMSE       Rsquared   MAE      
1                  100      0.4972855  0.2136409  0.3883217


# PCs - not very interesting, I don't think???

```{r}

r  <- prcomp(x = training %>% select(-response))

pd <- r$x %>% 
  as_tibble() %>%
  bind_cols(response=training$response) %>%
  {.}

ggplot(pd,aes(x=PC1, y=PC2, color=response))+
  geom_point() +
  xlim(0, 5000)

```




```{r}

# Boosting with 118 trees - final model round 3

# Always make the same split
set.seed(0)

# We use 80% for training and 20% for evaluation
trainfold <- data_train %>% sample_frac(size=0.80)
testfold  <- setdiff(data_train, trainfold)

# We fit our model to the training fold
#fit <- gbm(formula  = response ~ .,data = data_train, n.trees = 118)
fit <- gbm(formula  = response ~ .,data = data_train, n.trees = 118)

# We predict on the test fold
predicted <- predict(fit, newdata = testfold)

# We compare with the observed values and calculate RMSE
observed  <- testfold$response
mse       <- mean((observed-predicted)^2)
rmse     <- sqrt(mse)

test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)

test_rmse

```


```{r}

f <- gbm.step(data = training, gbm.x = 3:370, gbm.y = 2, n.trees = 118, family = "gaussian")

```


# Plot of testfold observed and predicted values and residuals
 

```{r}

pd  <- tibble(observed, predicted) %>% 
  mutate(residual = observed - predicted)


ggplot(pd, aes(x=observed, y=predicted)) + 
  geom_point() +
  theme_classic() +
  NULL


ggplot(pd, aes(x=observed, y=residual)) + 
  geom_point() +
  geom_hline(yintercept = 0, linetype="dashed") +
  theme_classic() +
  NULL

rm(pd)

```

# Predict the real unknown data

First we fit the model to all of our known data

Then we predict on the unknown data

The predictions must have the following column and the row order must be the same as the original!

* predicted (the predicted value)

```{r}

#fit <-  lm(formula  = response ~ 1, data = data_train)

#predicted <- predict(object=fit, newdata = data_predict, type = "response")

submission <- tibble(predicted)

head(submission)

```

# Submitting your answer

The following code will give us

* your chosen team name
* the name of the people on the team
* your estimated RMSE (from train/test or CV or similar)
* your predictions

Please edit the values below .

The filename of the output will be automated als_progression.TEAMNAME.rds

Please - do not use space or funny letters in your team name.

```{r}

team_name        <- "team_NA"
team_people      <- c("Nicklas", "Astrid")
team_error_rate  <- test_rmse
team_predictions <- submission # This should be a tibble with a column called "predicted"

#
# Always run this code
# If it fails you have done something wrong!
#
# Extract the columns needed
team_predictions <- team_predictions %>% select(predicted)

# Save all the stuff in one object
write_rds(x = list(team_name, team_people, team_error_rate, team_predictions), 
          file = paste("als_progression.", team_name, ".rds", sep=""))

```

# Checking format of all saved objects

```{r}

files   <- Sys.glob("als_progression.*.rds")
results <- tibble(filename = files, team_name=NA, team_people=NA, team_rmse=NA,n=NA, mean=NA)

for (i in 1:nrow(results)) {
  x <- read_rds(file = as.character(results$filename[i]))
  results$team_name[i]        <- x[[1]]
  results$team_people[i]      <- paste(x[[2]], collapse=",", sep=" ")
  results$team_rmse[i]        <- x[[3]]
  y                           <- x[[4]]
  results$n                   <- nrow(y)
  results$mean                <- mean(y$predicted, na.rm = T)
  results$submission[i]       <- list(x[[4]])
}

rm(x,y)

results %>% select(-filename)

results$submission[[1]]

```

# Upload your rds file!


