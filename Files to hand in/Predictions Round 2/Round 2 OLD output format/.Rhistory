minitcga_cancer_classification.team_NA <- readRDS("G:/My Drive/Bioinformatik/2022/SMLiB - R Exercises/Prediction handins/Files to hand in/Predictions Round 2/minitcga_cancer_classification.team_NA.rds")
View(minitcga_cancer_classification.team_NA)
# Chunk 1
library(tidyverse)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
# Chunk 2
set.seed(0)
data_train    <- df %>% filter(!is.na(response))
data_predict  <- df %>% filter(is.na(response))
dim(data_predict)
dim(data_train)
# Chunk 3
# THE LASSO
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 1)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 1)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Predict
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5263135
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 1)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 4
pd  <- tibble(observed, predicted) %>%
mutate(residual = observed - predicted)
ggplot(pd, aes(x=observed, y=predicted)) +
geom_point() +
theme_classic() +
NULL
ggplot(pd, aes(x=observed, y=residual)) +
geom_point() +
geom_hline(yintercept = 0, linetype="dashed") +
theme_classic() +
NULL
rm(pd)
# Chunk 5
submission <- tibble(predicted)
head(submission)
# Chunk 6
team_name        <- "team_NA"
team_people      <- c("Nicklas", "Astrid")
team_error_rate  <- test_rmse
team_predictions <- submission
#
# Always run this code
# If it fails you have done something wrong!
#
# Extract the columns needed
team_predictions <- team_predictions %>% select(predicted)
# Save all the stuff in one object
write_rds(x = list(team_name, team_people, team_error_rate, team_predictions),
path = paste("als_progression.", team_name, ".rds", sep=""))
library(tidyverse)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
library(tidyverse)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
install.packages(tidyverse)
library(tidyverse)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
# Chunk 1
library(tidyverse)
library(glmnet)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
# Chunk 2
set.seed(0)
data_train    <- df %>% filter(!is.na(response))
data_train    <- na.omit(data_train)
data_predict  <- df %>% filter(is.na(response))
dim(data_predict)
dim(data_train)
# Chunk 3
# RIDGE REGRESSION
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 0)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 0)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Test MSE
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5422445
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 0)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 4
# Elastic net with alpha = 0.5 (combination of ridge and lasso)
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 0.5)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 0.5)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Predict
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5261383
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 1)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 5
# THE LASSO
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 1)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 1)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Predict
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5263135
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 1)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 6
pd  <- tibble(observed, predicted) %>%
mutate(residual = observed - predicted)
ggplot(pd, aes(x=observed, y=predicted)) +
geom_point() +
theme_classic() +
NULL
ggplot(pd, aes(x=observed, y=residual)) +
geom_point() +
geom_hline(yintercept = 0, linetype="dashed") +
theme_classic() +
NULL
rm(pd)
# Chunk 7
#fit <-  lm(formula  = response ~ 1, data = data_train)
#predicted <- predict(object=fit, newdata = data_predict, type = "response")
submission <- tibble(predicted)
head(submission)
# Chunk 8
team_name        <- "team_NA"
team_people      <- c("Nicklas", "Astrid")
team_error_rate  <- test_rmse
team_predictions <- submission
#
# Always run this code
# If it fails you have done something wrong!
#
# Extract the columns needed
team_predictions <- team_predictions %>% select(predicted)
# Save all the stuff in one object
write_rds(x = list(team_name, team_people, team_error_rate, team_predictions),
path = paste("als_progression.", team_name, ".rds", sep=""))
files   <- Sys.glob("als_progression.*.rds")
results <- tibble(filename = files, team_name=NA, team_people=NA, team_rmse=NA,n=NA, mean=NA)
for (i in 1:nrow(results)) {
x <- read_rds(path = as.character(results$filename[i]))
results$team_name[i]        <- x[[1]]
results$team_people[i]      <- paste(x[[2]], collapse=",", sep=" ")
results$team_rmse[i]        <- x[[3]]
y                           <- x[[4]]
results$n                   <- nrow(y)
results$mean                <- mean(y$predicted, na.rm = T)
}
rm(x,y)
results %>% select(-filename)
# Chunk 1
library(tidyverse)
library(glmnet)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
# Chunk 2
set.seed(0)
data_train    <- df %>% filter(!is.na(response))
data_train    <- na.omit(data_train)
data_predict  <- df %>% filter(is.na(response))
dim(data_predict)
dim(data_train)
# Chunk 3
# RIDGE REGRESSION
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 0)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 0)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Test MSE
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5422445
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 0)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 4
# Elastic net with alpha = 0.5 (combination of ridge and lasso)
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 0.5)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 0.5)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Predict
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5261383
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 1)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 5
# THE LASSO
# Always make the same split
set.seed(0)
x <- model.matrix(response ~ ., data_train)[, -1]
y <- data_train$response
# Randomly sample a subset of numbers between 1 and nrow(x)
# We use 80% for training and 20% for evaluation
train <- sample(1 : nrow(x), nrow(x) / 5)
test <- (-train)
# Train fit
fit <- glmnet(x[train, ], y[train], alpha = 1)
# Find best lambda using CV
cv_lambda <- cv.glmnet(x[train, ], y[train], alpha = 1)
#plot(cv_lambda)
best_lambda <- cv_lambda$lambda.min
# Predict
predicted <- predict(fit, s = best_lambda, newx = x[test, ])
# We compare with the observed values and calculate RMSE
observed  <- y[test]
mse       <- mean((observed-predicted)^2)
(rmse     <- sqrt(mse)) # 0.5263135
test_rmse <- rmse # Our guess on the general rmse of the model (very unprecise!)
# Fit model on all our data using found best lambda
fit <- glmnet(x, y, alpha = 1)
predicted <- predict(fit, s = best_lambda, newx = x)
# Chunk 6
pd  <- tibble(observed, predicted) %>%
mutate(residual = observed - predicted)
ggplot(pd, aes(x=observed, y=predicted)) +
geom_point() +
theme_classic() +
NULL
ggplot(pd, aes(x=observed, y=residual)) +
geom_point() +
geom_hline(yintercept = 0, linetype="dashed") +
theme_classic() +
NULL
rm(pd)
# Chunk 7
#fit <-  lm(formula  = response ~ 1, data = data_train)
#predicted <- predict(object=fit, newdata = data_predict, type = "response")
submission <- tibble(predicted)
head(submission)
# Chunk 8
team_name        <- "team_NA"
team_people      <- c("Nicklas", "Astrid")
team_error_rate  <- test_rmse
team_predictions <- submission
#
# Always run this code
# If it fails you have done something wrong!
#
# Extract the columns needed
team_predictions <- team_predictions %>% select(predicted)
# Save all the stuff in one object
write_rds(x = list(team_name, team_people, team_error_rate, team_predictions),
path = paste("als_progression.", team_name, ".rds", sep=""))
team_error_rate  <- test_rmse
library(tidyverse)
library(glmnet)
df <- read_rds(path = "datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
library(tidyverse)
library(glmnet)
df <- read_rds(path = "../datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
#df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
library(tidyverse)
library(glmnet)
#df <- read_rds(path = "../datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
df <- df %>% rename(response = dFRS)
head(df)
library(tidyverse)
library(glmnet)
#df <- read_rds(path = "../datasets/ALS_data_regression/ALS_progression_rate.1822x370.rds")
df <- read_rds(path = "ALS_progression_rate.1822x370.rds")  %>% tbl_df()
